{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an environment using relevant requirements.txt to satisfy all of the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the code\n",
    "\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import EmbeddingFunction\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_database_path_mac = \"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/chroma_data\"\n",
    "client = chromadb.Client(settings=Settings(persist_directory=persistence_database_path_mac, allow_reset=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Code for creating chroma collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1620\n"
     ]
    }
   ],
   "source": [
    "# Here we are reading our dataset file\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = \"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/data_json/2013pubmed.json\"\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    dataset = json.load(json_file)\n",
    "\n",
    "# Print the loaded dataset\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 493kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 68.5kB/s]\n",
      "README.md: 100%|██████████| 10.7k/10.7k [00:00<00:00, 16.5MB/s]\n",
      "config.json: 100%|██████████| 573/573 [00:00<00:00, 329kB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 99.4kB/s]\n",
      "data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 6.85MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 134M/134M [00:10<00:00, 12.4MB/s] \n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 123kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 111kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.60MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 352/352 [00:00<00:00, 573kB/s]\n",
      "train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 14.0MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 82.4MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 769kB/s]\n"
     ]
    }
   ],
   "source": [
    "persistence_database_path_windows = \"chatbot_ic/lib/backend/data/chroma_data\"\n",
    "\n",
    "## Creating client instance of chroma db\n",
    "# client = chromadb.PersistentClient(path=\"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/chroma_data\", se)\n",
    "# client = chromadb.Client(settings=Settings(persist_directory=persistence_database_path_windows, allow_reset=True))\n",
    "\n",
    "## Initialising a sentence transformer for chromadb \n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "collection_2013 = client.create_collection(\"2013pubmed\", embedding_function=sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "# Extract data from the dataset and store it in the collection\n",
    "# collection_2013.add(\n",
    "#     ids=[str(entry['PMID']) for entry in dataset],\n",
    "#     documents=[entry['Abstract'] for entry in dataset],\n",
    "#     metadatas=[\n",
    "#         {'title': entry['Title'], 'author': entry['Author']} for entry in dataset\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "## adding data to chroma db\n",
    "collection_2013.add(\n",
    "    ids=[str(entry['PMID']) for entry in dataset],\n",
    "    documents=[entry['Title'] + \"<SEP>\" + entry['Abstract'] for entry in dataset],\n",
    "    metadatas=[\n",
    "        {'author': entry['Author']} for entry in dataset\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.13.3\n",
      "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai==1.13.3) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai==1.13.3) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai==1.13.3) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai==1.13.3) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai==1.13.3) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai==1.13.3) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai==1.13.3) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.13.3) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.13.3) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.13.3) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.13.3) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.13.3) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.13.3) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.13.3) (2.14.6)\n",
      "Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed openai-1.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YourCustomRetriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Create MultiQueryRetriever instance or provide the correct retriever instance here\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m retriever \u001b[38;5;241m=\u001b[39m MultiQueryRetriever(retriever\u001b[38;5;241m=\u001b[39m\u001b[43mYourCustomRetriever\u001b[49m(collection_2013), llm_chain\u001b[38;5;241m=\u001b[39mllm_chain, parser_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Generate multiple queries based on the context\u001b[39;00m\n\u001b[1;32m     67\u001b[0m generated_queries \u001b[38;5;241m=\u001b[39m generate_similar_queries(context)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YourCustomRetriever' is not defined"
     ]
    }
   ],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# OPEN_API_KEY = \"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\"\n",
    "\n",
    "# # Specify the model to use for query generation\n",
    "# client = OpenAI(api_key=OPEN_API_KEY)\n",
    "\n",
    "# # Define your context\n",
    "# context = \"Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on “distance”. But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\"\n",
    "\n",
    "# # Generate similar queries\n",
    "# def generate_similar_queries(original_query):\n",
    "#     response = client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates multiple search queries based on a single input query.\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"Generate multiple search queries related to: {original_query}\"},\n",
    "#             {\"role\": \"user\", \"content\": \"OUTPUT (4 queries):\"}\n",
    "#         ],\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#     )\n",
    "#     generated_queries = [choice.message.content for choice in response.choices]\n",
    "#     return generated_queries\n",
    "\n",
    "# # Vector search\n",
    "# def vector_search(retriever, query):\n",
    "#     search_results = {}\n",
    "#     retrieved_docs = retriever.get_relevant_documents(query)\n",
    "#     for i in retrieved_docs:\n",
    "#         search_results[i.page_content] = i.metadata['_additional']['certainty']\n",
    "#     return search_results\n",
    "\n",
    "# # Reciprocal Rank Fusion\n",
    "# def reciprocal_rank_fusion(search_results_dict, k=60):\n",
    "#     fused_scores = {}\n",
    "#     for query, doc_scores in search_results_dict.items():\n",
    "#         for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n",
    "#             if doc not in fused_scores:\n",
    "#                 fused_scores[doc] = 0\n",
    "#             previous_score = fused_scores[doc]\n",
    "#             fused_scores[doc] += 1 / (rank + k)\n",
    "#             print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n",
    "\n",
    "#     reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "#     print(\"Final reranked results:\", reranked_results)\n",
    "#     return reranked_results\n",
    "\n",
    "# # Generate output\n",
    "# def generate_output(original_query, reranked_results):\n",
    "#     reranked_docs = [i for i in reranked_results.keys()]\n",
    "#     context = '\\n'.join(reranked_docs)\n",
    "#     response = client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user's questions based on the context provided.\\nDo not make up an answer if you do not know it, stay within the bounds of the context provided, if you don't know the answer, say that you don't have enough information on the topic!\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"CONTEXT: {context}\\nQUERY: {original_query}\"},\n",
    "#             {\"role\": \"user\", \"content\": \"ANSWER:\"}\n",
    "#         ],\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#     )\n",
    "\n",
    "#     response = response.choices[0].delta.content.strip()\n",
    "#     return response\n",
    "\n",
    "# # Create MultiQueryRetriever instance or provide the correct retriever instance here\n",
    "# retriever = MultiQueryRetriever(retriever=YourCustomRetriever(collection_2013), llm_chain=llm_chain, parser_key=\"lines\")\n",
    "\n",
    "# # Generate multiple queries based on the context\n",
    "# generated_queries = generate_similar_queries(context)\n",
    "\n",
    "# # Retrieve relevant documents\n",
    "# all_results = {}\n",
    "# for query in generated_queries:\n",
    "#     search_results = vector_search(retriever, query)\n",
    "#     all_results[query] = search_results\n",
    "\n",
    "# # Perform reciprocal rank fusion\n",
    "# reranked_result = reciprocal_rank_fusion(all_results)\n",
    "\n",
    "# # Generate the final output\n",
    "# final_output = generate_output(context, reranked_result)\n",
    "# print(f\"Generated Response -> {final_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for creating collections of all the available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create multiple collections\n",
    "persistence_database_path_windows = \"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_mac = \"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/chroma_data\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=persistence_database_path_windows, settings=Settings(allow_reset=True))\n",
    "\n",
    "def create_chroma_collections(years):\n",
    "    \n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"multi-qa-MiniLM-L6-cos-v1\", normalize_embeddings=True)\n",
    "    \n",
    "    for year in years:\n",
    "        # Specify the path to your JSON file\n",
    "        json_file_path = f\"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "\n",
    "        # Open the JSON file and load the data\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            dataset = json.load(json_file)\n",
    "            \n",
    "        collection = client.create_collection(f\"{year}pubmed\", embedding_function=sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "        \n",
    "        collection.add(\n",
    "            ids=[str(entry['PMID']) for entry in dataset],\n",
    "            documents=[entry['Title'] + \"<SEP>\" + entry['Abstract'] for entry in dataset],\n",
    "            metadatas=[\n",
    "                {'author': entry['Author']} for entry in dataset\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2013', '2014', '2015', '2016-2017', '2018', '2019', '2020-1', '2020-2']\n",
    "\n",
    "create_chroma_collections(years=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=2013pubmed)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection('2013pubmed')\n",
    "client.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
