{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import EmbeddingFunction\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Terminal command to start a local host server\n",
    "## chroma run --path \"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/chroma_data\"\n",
    "\n",
    "## instantiate chroma client\n",
    "chroma_internet_client = chromadb.HttpClient(host='16.171.68.145', port=8000, settings=Settings(allow_reset=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create multiple collections\n",
    "persistence_database_path_windows = \"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_mac = \"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_ubuntu = \"/home/ubuntu/chromadb\"\n",
    "\n",
    "## for local persistent database\n",
    "# chroma_internet_client = chromadb.PersistentClient(path=persistence_database_path_mac, settings=Settings(allow_reset=True))\n",
    "\n",
    "\n",
    "def create_chroma_collections(years):\n",
    "    \n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "    \n",
    "    for year in years:\n",
    "        # Specify the path to your JSON file\n",
    "        #json_file_path = f\"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "        json_file_path =f\"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "        # Open the JSON file and load the data\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            dataset = json.load(json_file)\n",
    "            \n",
    "        collection = chroma_internet_client.create_collection(f\"{year}pubmed\", embedding_function=sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "        \n",
    "        collection.add(\n",
    "            ids=[str(entry['PMID']) for entry in dataset],\n",
    "            documents= [ str(entry['PMID']) + \"<SEP>\" +  entry['Author'] + \"<SEP>\" + entry['Title'] + \"<SEP>\" + entry['Abstract'] for entry in dataset],\n",
    "            metadatas=[\n",
    "                {'author': entry['Author']} for entry in dataset\n",
    "            ],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For creating a large collection\n",
    "\n",
    "# Function to create multiple collections\n",
    "persistence_database_path_windows = \"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_mac = \"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_ubuntu = \"/home/ubuntu/chromadb\"\n",
    "\n",
    "## for local persistent database\n",
    "# chroma_internet_client = chromadb.PersistentClient(path=persistence_database_path_mac, settings=Settings(allow_reset=True))\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "\n",
    "collection_large = chroma_internet_client.create_collection(f\"pubmed_whole\", embedding_function=sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "\n",
    "def create_whole_chroma_collections(years):\n",
    "    \n",
    "    \n",
    "    \n",
    "    for year in years:\n",
    "        # Specify the path to your JSON file\n",
    "        json_file_path = f\"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "        # json_file_path =f\"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "        # Open the JSON file and load the data\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            dataset = json.load(json_file)\n",
    "            \n",
    "        \n",
    "        collection_large.add(\n",
    "            ids=[str(entry['PMID']) for entry in dataset],\n",
    "            documents= [ str(entry['PMID']) + \"<SEP>\" +  entry['Author'] + \"<SEP>\" + entry['Title'] + \"<SEP>\" + entry['Abstract'] for entry in dataset],\n",
    "            metadatas=[\n",
    "                {'author': entry['Author']} for entry in dataset\n",
    "            ],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 12,
=======
   "execution_count": 8,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2013', '2014', '2015', '2016-2017', '2018','2019', '2020-1', '2020-2','2021','2022','2023']\n",
    "\n",
    "create_whole_chroma_collections(years=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=2018pubmed),\n",
       " Collection(name=pubmed_whole),\n",
       " Collection(name=2015pubmed),\n",
       " Collection(name=2020-2pubmed),\n",
       " Collection(name=2016-2017pubmed),\n",
       " Collection(name=2022pubmed),\n",
       " Collection(name=2020-1pubmed),\n",
       " Collection(name=2021pubmed),\n",
       " Collection(name=2013pubmed),\n",
       " Collection(name=2019pubmed),\n",
       " Collection(name=2014pubmed),\n",
       " Collection(name=2023pubmed)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## access 2013 collection\n",
    "chroma_internet_client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code for retrieving top article based on the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question:\n",
      "CASK Disorder\n",
      "\n",
      "Context with Maximum Similarity Score:\n",
      "23944117<SEP>Yoko Kamio<SEP>Psychiatric issues of children and adults with autism spectrum disorders who remain undiagnosed]<SEP>Individuals of normal intelligence with autism spectrum disorders (ASD) tend to be diagnosed with ASD late in childhood or sometimes in adulthood, despite a persistent symptomatology. When such patients visit psychiatric clinics for co-occurring psychiatric symptoms, the diagnostic procedure can be challenging due to a lack of accurate developmental information and a mixed clinical presentation. The same is true for those with subthreshold autistic symptoms. Although individuals with subthreshold ASD also have social adjustment difficulties of a similar degree to those with ASD, the relative clinical significance of this population is unclear. Here, data from a large national population sample of schoolchildren were examined to determine the psychiatric needs of children with threshold and subthreshold autistic symptoms. First, autistic symptoms or traits assessed by the Social Responsiveness Scale (SRS), a quantitative behavioral measure, showed a continuous distribution in the general child population (n = 22,529), indicating no evidence of a natural gap that could differentiate children diagnosed with ASD from subthreshold or unaffected children. Second, data from 25,075 children demonstrated that having threshold autistic symptoms predicted a high psychiatric risk, as indicated by higher scores on the Strengths and Difficulties Questionnaire (SDQ; odds ratio [OR] 200.52, 95% confidence interval [CI]: 152.12-264.33), and that having subthreshold autistic symptoms indicated the same (OR 12.78, 95% CI: 11.52-14.18). Having threshold autistic symptoms predicted emotional problems (OR 20.19, 95% CI: 17.00-24.00), as did having subthreshold autistic symptoms (OR 5.90, 95% CI: 5.29-6.58). Third, among 2,250 children at a high psychiatric risk, most had threshold or subthreshold autistic symptoms (21 and 44%, respectively). These findings have important implications for the comprehensive psychiatric and developmental evaluation and treatment of this patient population, whose diagnosis and treatment are often delayed, and a further in-depth study is warranted.\n",
      "Similarity Score: 0.6682615280151367\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "\n",
    "# User's question\n",
    "user_question = 'CASK Disorder'\n",
    "\n",
    "# Embed the user's question\n",
    "user_question_embedding = sentence_transformer_ef([user_question])[0]\n",
    "\n",
    "search_results = collection_2013.query(query_embeddings=[user_question_embedding], n_results=5)\n",
    "\n",
    "# Find the index of the context with the maximum similarity score\n",
    "max_similarity_index = search_results['distances'][0].index(max(search_results['distances'][0]))\n",
    "\n",
    "# Get the context with the maximum similarity score\n",
    "context_with_max_similarity = search_results['documents'][0][max_similarity_index]\n",
    "\n",
    "print(\"User Question:\")\n",
    "print(user_question)\n",
    "print(\"\\nContext with Maximum Similarity Score:\")\n",
    "print(context_with_max_similarity)\n",
    "print(\"Similarity Score:\", max(search_results['distances'][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now you can feed the user question and the context with maximum similarity score into the Turbo model\n",
    "def get_context_with_max_similarity(user_question):\n",
    "    # Initialize SentenceTransformer model\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "\n",
    "    # Embed the user's question\n",
    "    user_question_embedding = sentence_transformer_ef([user_question])[0]\n",
    "\n",
    "    # Perform the query using Chroma\n",
    "    search_results = collection_2013.query(query_embeddings=[user_question_embedding], n_results=5)\n",
    "\n",
    "    # Find the index of the context with the maximum similarity score\n",
    "    max_similarity_index = search_results['distances'][0].index(max(search_results['distances'][0]))\n",
    "\n",
    "    # Get the context with the maximum similarity score\n",
    "    context_with_max_similarity = search_results['documents'][0][max_similarity_index]\n",
    "\n",
    "    return context_with_max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  CASK Disorder\n",
      "-------------------------\n",
      "Answer:  \n",
      "CASK (CASK disorder) is a neurodevelopmental disorder that falls under the autism spectrum disorder (ASD) umbrella. It is characterized by difficulties with social skills and communication, as well as other symptoms that overlap with ASD. It is often diagnosed later in childhood or even in adulthood, despite persistent symptoms. Individuals with CASK may also have co-occurring psychiatric disorders, making diagnosis and treatment challenging. Data from a large study showed that individuals with CASK or subthreshold autistic symptoms have a high psychiatric risk and may require comprehensive evaluation and treatment. This highlights the importance of early identification and intervention for individuals with CASK. \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "sentence_transformer_ef = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# Turbo setup\n",
    "OPEN_API_KEY = \"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\"\n",
    "llm = OpenAI(api_key=OPEN_API_KEY)\n",
    "QUERY_PROMPT_MODEL = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"\"\"\n",
    "You are an AI language model assistant. Your task is to generate answer\n",
    "    by taking information from the relevant context provided from a vector \n",
    "    database. By considering multiple perspectives on the user question, your goal is to help\n",
    "    the user understand the concept of the question asked that is also relevant to the context provided. \n",
    "    Provide these answers with proper type setting.\n",
    "\n",
    "    Original question: {question}\n",
    "    Context : {context}\n",
    "\"\"\",)\n",
    "\n",
    "MULTI_QUERY_PROMPT_MODEL = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate questions based on the provided context. \n",
    "    Generate list of 3 questions that help to explore different aspects of the context and deepen the understanding of the topic. \n",
    "    Provide these questions with proper type setting.\n",
    "\n",
    "    \n",
    "    Context : {context}\n",
    "\"\"\",)\n",
    "\n",
    "\n",
    "\n",
    "llm_chain_multi_query= LLMChain(prompt=MULTI_QUERY_PROMPT_MODEL, llm=llm)\n",
    "llm_chain_answer = LLMChain(prompt=QUERY_PROMPT_MODEL, llm=llm)\n",
    "\n",
    "def generate_questions(context):\n",
    "    # Generate questions based on the provided context\n",
    "    generated_questions = llm_chain_multi_query.run({'context': context})\n",
    "    \n",
    "    return generated_questions\n",
    "\n",
    "\n",
    "context_with_max_similarity = get_context_with_max_similarity(user_question)\n",
    "\n",
    "generated_questions = generate_questions(context_with_max_similarity)\n",
    "\n",
    "#FETCHING 3 questions\n",
    "\n",
    "# Split the generated questions string into a list of individual questions\n",
    "generated_questions_list = generated_questions.split('\\n')\n",
    "\n",
    "# Filter out any empty strings from the list\n",
    "generated_questions_list = [question.strip() for question in generated_questions_list if question.strip()]\n",
    "\n",
    "# Extract the first three questions from the list\n",
    "question1 = generated_questions_list[0] if len(generated_questions_list) > 0 else \"\"\n",
    "question2 = generated_questions_list[1] if len(generated_questions_list) > 1 else \"\"\n",
    "question3 = generated_questions_list[2] if len(generated_questions_list) > 2 else \"\"\n",
    "\n",
    "\n",
    "\n",
    "q1_with_max_simialrity=get_context_with_max_similarity(question1)\n",
    "q2_with_max_simialrity=get_context_with_max_similarity(question2)\n",
    "q3_with_max_simialrity=get_context_with_max_similarity(question3)\n",
    "\n",
    "\n",
    "\n",
    "new_context=context_with_max_similarity+q1_with_max_simialrity+q2_with_max_simialrity+q3_with_max_simialrity\n",
    "# print(\"ANSWER\")\n",
    "answer = llm_chain_answer.run({'question': user_question,'context': new_context, })\n",
    "print(\"Question: \",user_question)\n",
    "print(\"-------------------------\")\n",
    "\n",
    "print(\"Answer: \",answer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=2018pubmed),\n",
       " Collection(name=2015pubmed),\n",
       " Collection(name=2020-2pubmed),\n",
       " Collection(name=2016-2017pubmed),\n",
       " Collection(name=2022pubmed),\n",
       " Collection(name=2020-1pubmed),\n",
       " Collection(name=2021pubmed),\n",
       " Collection(name=2013pubmed),\n",
       " Collection(name=2019pubmed),\n",
       " Collection(name=2014pubmed),\n",
       " Collection(name=2023pubmed)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_internet_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_internet_client.delete_collection(\"2023pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
