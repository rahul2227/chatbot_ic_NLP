{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import EmbeddingFunction\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Terminal command to start a local host server\n",
    "## chroma run --path \"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/chroma_data\"\n",
    "\n",
    "## instantiate chroma client\n",
    "chroma_internet_client = chromadb.HttpClient(host='16.171.68.145', port=8000, settings=Settings(allow_reset=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create multiple collections\n",
    "persistence_database_path_windows = \"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_mac = \"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_ubuntu = \"/home/ubuntu/chromadb\"\n",
    "\n",
    "## for local persistent database\n",
    "# chroma_internet_client = chromadb.PersistentClient(path=persistence_database_path_mac, settings=Settings(allow_reset=True))\n",
    "\n",
    "\n",
    "def create_chroma_collections(years):\n",
    "    \n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "    \n",
    "    for year in years:\n",
    "        # Specify the path to your JSON file\n",
    "        #json_file_path = f\"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "        json_file_path =f\"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "        # Open the JSON file and load the data\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            dataset = json.load(json_file)\n",
    "            \n",
    "        collection = chroma_internet_client.create_collection(f\"{year}pubmed\", embedding_function=sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "        \n",
    "        collection.add(\n",
    "            ids=[str(entry['PMID']) for entry in dataset],\n",
    "            documents= [ str(entry['PMID']) + \"<SEP>\" +  entry['Author'] + \"<SEP>\" + entry['Title'] + \"<SEP>\" + entry['Abstract'] for entry in dataset],\n",
    "            metadatas=[\n",
    "                {'author': entry['Author']} for entry in dataset\n",
    "            ],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For creating a large collection\n",
    "\n",
    "# Function to create multiple collections\n",
    "persistence_database_path_windows = \"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_mac = \"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/chroma_data\"\n",
    "\n",
    "persistence_database_path_ubuntu = \"/home/ubuntu/chromadb\"\n",
    "\n",
    "## for local persistent database\n",
    "# chroma_internet_client = chromadb.PersistentClient(path=persistence_database_path_mac, settings=Settings(allow_reset=True))\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "\n",
    "collection_large = chroma_internet_client.create_collection(f\"pubmed_whole\", embedding_function=sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "\n",
    "def create_whole_chroma_collections(years):\n",
    "    \n",
    "    \n",
    "    \n",
    "    for year in years:\n",
    "        # Specify the path to your JSON file\n",
    "        json_file_path = f\"G:/All Flutter Applications/NLP with transformers project/chatbot_ic/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "        # json_file_path =f\"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/data/data_json/{year}pubmed.json\"\n",
    "        # Open the JSON file and load the data\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            dataset = json.load(json_file)\n",
    "            \n",
    "        \n",
    "        collection_large.add(\n",
    "            ids=[str(entry['PMID']) for entry in dataset],\n",
    "            documents= [ str(entry['PMID']) + \"<SEP>\" +  entry['Author'] + \"<SEP>\" + entry['Title'] + \"<SEP>\" + entry['Abstract'] for entry in dataset],\n",
    "            metadatas=[\n",
    "                {'author': entry['Author']} for entry in dataset\n",
    "            ],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2013', '2014', '2015', '2016-2017', '2018','2019', '2020-1', '2020-2','2021','2022','2023']\n",
    "\n",
    "create_whole_chroma_collections(years=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=2018pubmed),\n",
       " Collection(name=pubmed_whole),\n",
       " Collection(name=2015pubmed),\n",
       " Collection(name=2020-2pubmed),\n",
       " Collection(name=2016-2017pubmed),\n",
       " Collection(name=2022pubmed),\n",
       " Collection(name=2020-1pubmed),\n",
       " Collection(name=2021pubmed),\n",
       " Collection(name=2013pubmed),\n",
       " Collection(name=2019pubmed),\n",
       " Collection(name=2014pubmed),\n",
       " Collection(name=2023pubmed)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## access 2013 collection\n",
    "chroma_internet_client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for getting top 5 articles based on the similarity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"multi-qa-MiniLM-L6-cos-v1\", normalize_embeddings=True)\n",
    "\n",
    "# User's question\n",
    "user_question = 'CASK Disorder'\n",
    "\n",
    "# Embed the user's question\n",
    "user_question_embedding = sentence_transformer_ef([user_question])[0]\n",
    "\n",
    "collection = chroma_internet_client.get_collection(\"pubmed_whole\")\n",
    "\n",
    "search_results = collection.query(query_embeddings=[user_question_embedding], n_results=5)\n",
    "print(user_question)\n",
    "# Print the search results\n",
    "for i, result in enumerate(search_results['ids'][0]):\n",
    "    document_id = result\n",
    "    metadata = search_results['metadatas'][0][i]  # Access the corresponding metadata\n",
    "    similarity_score = search_results['distances'][0][i]  # Access the corresponding similarity score\n",
    "    document = search_results['documents'][0][i]  # Access the corresponding document\n",
    "\n",
    "    print(f\"PMID: {document_id}\")\n",
    "    # print(f\"Title: {metadata['title']}\")\n",
    "    print(f\"Author: {metadata['author']}\")\n",
    "    print(document)\n",
    "    print(f\"Similarity Score: {similarity_score}\")\n",
    "    print(\"---------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code for retrieving top article based on the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vasu/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question:\n",
      "CASK Disorder\n",
      "\n",
      "Context with Maximum Similarity Score:\n",
      "23944117<SEP>Yoko Kamio<SEP>Psychiatric issues of children and adults with autism spectrum disorders who remain undiagnosed]<SEP>Individuals of normal intelligence with autism spectrum disorders (ASD) tend to be diagnosed with ASD late in childhood or sometimes in adulthood, despite a persistent symptomatology. When such patients visit psychiatric clinics for co-occurring psychiatric symptoms, the diagnostic procedure can be challenging due to a lack of accurate developmental information and a mixed clinical presentation. The same is true for those with subthreshold autistic symptoms. Although individuals with subthreshold ASD also have social adjustment difficulties of a similar degree to those with ASD, the relative clinical significance of this population is unclear. Here, data from a large national population sample of schoolchildren were examined to determine the psychiatric needs of children with threshold and subthreshold autistic symptoms. First, autistic symptoms or traits assessed by the Social Responsiveness Scale (SRS), a quantitative behavioral measure, showed a continuous distribution in the general child population (n = 22,529), indicating no evidence of a natural gap that could differentiate children diagnosed with ASD from subthreshold or unaffected children. Second, data from 25,075 children demonstrated that having threshold autistic symptoms predicted a high psychiatric risk, as indicated by higher scores on the Strengths and Difficulties Questionnaire (SDQ; odds ratio [OR] 200.52, 95% confidence interval [CI]: 152.12-264.33), and that having subthreshold autistic symptoms indicated the same (OR 12.78, 95% CI: 11.52-14.18). Having threshold autistic symptoms predicted emotional problems (OR 20.19, 95% CI: 17.00-24.00), as did having subthreshold autistic symptoms (OR 5.90, 95% CI: 5.29-6.58). Third, among 2,250 children at a high psychiatric risk, most had threshold or subthreshold autistic symptoms (21 and 44%, respectively). These findings have important implications for the comprehensive psychiatric and developmental evaluation and treatment of this patient population, whose diagnosis and treatment are often delayed, and a further in-depth study is warranted.\n",
      "Similarity Score: 0.6682615280151367\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "\n",
    "# User's question\n",
    "user_question = 'CASK Disorder'\n",
    "\n",
    "# Embed the user's question\n",
    "user_question_embedding = sentence_transformer_ef([user_question])[0]\n",
    "\n",
    "collection = chroma_internet_client.get_collection(\"pubmed_whole\")\n",
    "\n",
    "search_results = collection.query(query_embeddings=[user_question_embedding], n_results=5)\n",
    "# Find the index of the context with the maximum similarity score\n",
    "max_similarity_index = search_results['distances'][0].index(max(search_results['distances'][0]))\n",
    "\n",
    "# Get the context with the maximum similarity score\n",
    "context_with_max_similarity = search_results['documents'][0][max_similarity_index]\n",
    "\n",
    "print(\"User Question:\")\n",
    "print(user_question)\n",
    "print(\"\\nContext with Maximum Similarity Score:\")\n",
    "print(context_with_max_similarity)\n",
    "print(\"Similarity Score:\", max(search_results['distances'][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Now you can feed the user question and the context with maximum similarity score into the Turbo model\n",
    "def get_context_with_max_similarity(user_question):\n",
    "    # Initialize SentenceTransformer model\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L12-v2\", normalize_embeddings=True)\n",
    "\n",
    "    # Embed the user's question\n",
    "    user_question_embedding = sentence_transformer_ef([user_question])[0]\n",
    "\n",
    "    # Perform the query using Chroma\n",
    "    collection = chroma_internet_client.get_collection(\"pubmed_whole\")\n",
    "\n",
    "    search_results = collection.query(query_embeddings=[user_question_embedding], n_results=5)\n",
    "\n",
    "    # Find the index of the context with the maximum similarity score\n",
    "    max_similarity_index = search_results['distances'][0].index(max(search_results['distances'][0]))\n",
    "\n",
    "    # Get the context with the maximum similarity score\n",
    "    context_with_max_similarity = search_results['documents'][0][max_similarity_index]\n",
    "\n",
    "    return context_with_max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=2018pubmed),\n",
       " Collection(name=2015pubmed),\n",
       " Collection(name=2020-2pubmed),\n",
       " Collection(name=2016-2017pubmed),\n",
       " Collection(name=2022pubmed),\n",
       " Collection(name=2020-1pubmed),\n",
       " Collection(name=2021pubmed),\n",
       " Collection(name=2013pubmed),\n",
       " Collection(name=2019pubmed),\n",
       " Collection(name=2014pubmed),\n",
       " Collection(name=2023pubmed)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_internet_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_internet_client.delete_collection(\"2023pubmed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
