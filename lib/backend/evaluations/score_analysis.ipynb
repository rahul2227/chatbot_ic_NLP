{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Scores for Celio Model:\n",
      "Grammar: 0.0\n",
      "Coherence: 0.032507739938080496\n",
      "Context: 0.5954105359457192\n",
      "Correctness: 0.032507739938080496\n",
      "\n",
      "Evaluation Scores for GPT-4 Model:\n",
      "Grammar: 0.0\n",
      "Coherence: 0.009287925696594427\n",
      "Context: 0.5954105359457192\n",
      "Correctness: 0.009287925696594427\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_similarity_score(text1, text2):\n",
    "    return SequenceMatcher(None, text1, text2).ratio()\n",
    "\n",
    "def evaluate_answers(abc_answer, xyz_answer):\n",
    "    scores = {\n",
    "        \"Grammar\": 0.0,\n",
    "        \"Coherence\": 0.0,\n",
    "        \"Context\": 0.0,\n",
    "        \"Correctness\": 0.0,\n",
    "    }\n",
    "\n",
    "    # Grammar Score (based on grammatical correctness)\n",
    "    scores[\"Grammar\"] = 1.0 if nltk.edit_distance(abc_answer['answer'], xyz_answer['answer']) == 0 else 0.0\n",
    "\n",
    "    # Coherence Score (based on logical consistency)\n",
    "    scores[\"Coherence\"] = calculate_similarity_score(abc_answer['answer'], xyz_answer['answer'])\n",
    "\n",
    "    # Context Score (based on relevance to the provided context)\n",
    "    context_score = calculate_similarity_score(abc_answer['context'], xyz_answer['context'])\n",
    "    scores[\"Context\"] = context_score\n",
    "\n",
    "    # Correctness Score (based on factual accuracy)\n",
    "    correctness_score = calculate_similarity_score(abc_answer['answer'], xyz_answer['answer'])\n",
    "    scores[\"Correctness\"] = correctness_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Load data from JSON files\n",
    "with open(\"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/evaluations/generated_gpt3.5_context_and_answers.json\", \"r\") as cellio_file:\n",
    "    cellio = json.load(cellio_file)\n",
    "\n",
    "with open(\"/Users/vasu/Desktop/NLP /project/chatbot_ic_NLP/lib/backend/evaluations/generated_gpt-4_context_and_answers.json\", \"r\") as gpt4_file:\n",
    "    gpt4 = json.load(gpt4_file)\n",
    "\n",
    "# Assuming both files have answers to the same question with the same ID\n",
    "question_id = cellio[0]['id']\n",
    "\n",
    "# Find answers for the same question ID\n",
    "celio_answer = next(item for item in cellio if item[\"id\"] == question_id)\n",
    "gpt4_answer = next(item for item in gpt4 if item[\"id\"] == question_id)\n",
    "\n",
    "# Evaluate answers\n",
    "# Evaluate answers for Celio model\n",
    "evaluation_scores_celio = evaluate_answers(celio_answer, gpt4_answer)\n",
    "\n",
    "# Evaluate answers for GPT-4 model\n",
    "evaluation_scores_gpt4 = evaluate_answers(gpt4_answer, celio_answer)\n",
    "\n",
    "# Print evaluation scores for Celio model\n",
    "print(\"Evaluation Scores for Celio Model:\")\n",
    "for criterion, score in evaluation_scores_celio.items():\n",
    "    print(f\"{criterion}: {score}\")\n",
    "\n",
    "# Print evaluation scores for GPT-4 model\n",
    "print(\"\\nEvaluation Scores for GPT-4 Model:\")\n",
    "for criterion, score in evaluation_scores_gpt4.items():\n",
    "    print(f\"{criterion}: {score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt3.5\n",
      "    Grammar : 0.0/10\n",
      "    Coherence : 0.0/10\n",
      "    Context : 0.0/10\n",
      "    Correctness : 0.0/10\n",
      "\n",
      "gpt-4\n",
      "    Grammar : 0.0/10\n",
      "    Coherence : 0.0/10\n",
      "    Context : 0.0/10\n",
      "    Correctness : 0.0/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"healio\"\n",
    "# model_id = \"gpt-3.5-turbo\"\n",
    "for model_id in [\"gpt3.5\", \"gpt-4\"]:\n",
    "    with open(f\"generated_{model_id}_context_and_answers.json\", \"r\") as f:\n",
    "        evaluations = json.load(f)\n",
    "    scores = {\n",
    "        \"Grammar\": 0.0,\n",
    "        \"Coherence\": 0.0,\n",
    "        \"Context\": 0.0,\n",
    "        \"Correctness\": 0.0,\n",
    "    }\n",
    "    for q in evaluations:\n",
    "        e = q[\"answer\"]\n",
    "        if \"/10\" in e:\n",
    "            e = e.strip().replace(\"\\n\", \"\").replace(\"/10\", \"/10 \").replace(\".\", \" \").replace(\":\", \" \").replace(\"(\", \" \").replace(\")\", \" \").split(\" \")\n",
    "            e = [w for w in e if w != \"\"]\n",
    "            for i, tok in enumerate(e):\n",
    "                if \"/10\" in tok:\n",
    "                    try:\n",
    "                        nom = float(tok.split(\"/\")[0])\n",
    "                        denom = float(tok.split(\"/\")[1])\n",
    "                        scores[e[i-1]] += nom / denom\n",
    "                    except Exception as ex:\n",
    "                        continue  # unnecessary total grades\n",
    "        else:\n",
    "            pass\n",
    "            # print(\"GR\n",
    "            # ADING FAILED!\")\n",
    "            # for k, v in q.items():\n",
    "            #     print(k, v)\n",
    "    print(model_id)\n",
    "    for k, v in scores.items():\n",
    "        print(f\"    {k} : {np.round(v/6, 2)}/10\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Scores:\n",
      "Grammar: 0.0\n",
      "Coherence: 0.054168465815008\n",
      "Context: 0.37148701603879514\n",
      "Correctness: 0.054168465815008\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_similarity_score(text1, text2):\n",
    "    return SequenceMatcher(None, text1, text2).ratio()\n",
    "\n",
    "def evaluate_answers(answers1, answers2):\n",
    "    scores = {\n",
    "        \"Grammar\": 0.0,\n",
    "        \"Coherence\": 0.0,\n",
    "        \"Context\": 0.0,\n",
    "        \"Correctness\": 0.0,\n",
    "    }\n",
    "\n",
    "    # Calculate Grammar Score (based on grammatical correctness)\n",
    "    grammar_score = sum(1 for answer1, answer2 in zip(answers1, answers2) if nltk.edit_distance(answer1['answer'], answer2['answer']) == 0)\n",
    "    scores[\"Grammar\"] = grammar_score / len(answers1)\n",
    "\n",
    "    # Calculate Coherence Score (based on logical consistency)\n",
    "    coherence_score = sum(calculate_similarity_score(answer1['answer'], answer2['answer']) for answer1, answer2 in zip(answers1, answers2)) / len(answers1)\n",
    "    scores[\"Coherence\"] = coherence_score\n",
    "\n",
    "    # Calculate Context Score (based on relevance to the provided context)\n",
    "    context_score = sum(calculate_similarity_score(answer1['context'], answer2['context']) for answer1, answer2 in zip(answers1, answers2)) / len(answers1)\n",
    "    scores[\"Context\"] = context_score\n",
    "\n",
    "    # Calculate Correctness Score (based on factual accuracy)\n",
    "    correctness_score = sum(calculate_similarity_score(answer1['answer'], answer2['answer']) for answer1, answer2 in zip(answers1, answers2)) / len(answers1)\n",
    "    scores[\"Correctness\"] = correctness_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Load data from JSON files\n",
    "with open(\"generated_gpt3.5_context_and_answers.json\", \"r\") as f:\n",
    "    answers_gpt3_5 = json.load(f)\n",
    "\n",
    "with open(\"generated_gpt-4_context_and_answers.json\", \"r\") as f:\n",
    "    answers_gpt4 = json.load(f)\n",
    "\n",
    "# Evaluate answers\n",
    "evaluation_scores = evaluate_answers(answers_gpt3_5, answers_gpt4)\n",
    "\n",
    "# Print evaluation scores\n",
    "print(\"Evaluation Scores:\")\n",
    "for criterion, score in evaluation_scores.items():\n",
    "    print(f\"{criterion}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
